{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Collab_test.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHCb55xeVY1T",
        "colab_type": "text"
      },
      "source": [
        "# Preparations\n",
        "\n",
        "- Mount Google Drive \n",
        "- Copy and extract data,\n",
        "- Checkout or pull to get the latest code from github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ET06QtscMHL",
        "colab_type": "code",
        "outputId": "2c7011a4-49e3-424f-e808-f531715449df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os, time\n",
        "\n",
        "colab_data_dir = \"/content/data/\"\n",
        "gdrive_workspace = r\"/gdrive/My Drive/colab_workspace/advertima/data\"\n",
        "\n",
        "if not os.path.exists(colab_data_dir):\n",
        "  !mkdir {colab_data_dir}\n",
        "  print(\"Data folder created.\")\n",
        "\n",
        "if not os.path.exists(\"/gdrive\"):\n",
        "  print(\"Mounting google drive\")\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "print(\"Google drive is mounted at /gdrive\")\n",
        "  \n",
        "traindata_prefix = \"300W_LP\"\n",
        "traindata_dir = os.path.join(colab_data_dir, traindata_prefix)\n",
        "testdata_prefix = \"AFLW2000\"\n",
        "testdata_dir = os.path.join(colab_data_dir, testdata_prefix)  \n",
        "  \n",
        "  \n",
        "start_time = time.time()\n",
        "if not os.path.exists(traindata_dir):  \n",
        "  print(\"Extracting {}...\".format(traindata_prefix))\n",
        "  !unzip -qq -n {os.path.join(gdrive_workspace, traindata_prefix) + \".zip\"} -d  {colab_data_dir}\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"{} is ready ({:.1f} sec)\".format(os.path.join(colab_data_dir, traindata_prefix), elapsed_time))\n",
        "  \n",
        "start_time = time.time()\n",
        "if not os.path.exists(testdata_dir):  \n",
        "  print(\"Extracting {}...\".format(testdata_prefix)),\n",
        "  !unzip -qq -n {os.path.join(gdrive_workspace, testdata_prefix) + \".zip\"} -d  {colab_data_dir}\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"{} is ready ({:.1f} sec)\".format(os.path.join(colab_data_dir, testdata_prefix), elapsed_time))\n",
        "\n",
        "!ls {colab_data_dir}"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google drive is mounted at /gdrive\n",
            "/content/data/300W_LP is ready (0.0 sec)\n",
            "/content/data/AFLW2000 is ready (0.0 sec)\n",
            "300W_LP  300W_LP_filenames.txt\tAFLW2000  AFLW2000_filenames.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFBIQBoOl2Gd",
        "colab_type": "text"
      },
      "source": [
        "**Code from github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL4AkKu4h_fB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "47813924-eed0-430e-c98c-2c30ed84c514"
      },
      "source": [
        "%cd /content\n",
        "if not os.path.exists(\"/content/deep-head-pose\"):\n",
        "  # Clone the entire repo.\n",
        "  !git clone -l -s https://github.com/emredog/deep-head-pose.git deep-head-pose\n",
        "  %cd deep-head-pose/code\n",
        "else:\n",
        "  %cd deep-head-pose/code\n",
        "  !git pull"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/deep-head-pose/code\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/emredog/deep-head-pose\n",
            "   f0b4929..3caab65  master     -> origin/master\n",
            "Updating f0b4929..3caab65\n",
            "Fast-forward\n",
            " code/train_hopenet.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLLB0RHelwl1",
        "colab_type": "text"
      },
      "source": [
        "**Fetch or generate filenames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQlukpSrlwNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "48364802-d1b4-4ff6-cec2-a2411e248d49"
      },
      "source": [
        "from shutil import copyfile\n",
        "import utils\n",
        "       \n",
        "  \n",
        "\n",
        "for prefix in [traindata_prefix, testdata_prefix]:\n",
        "  if not os.path.exists(os.path.join(colab_data_dir, prefix + \"_filenames.txt\")):\n",
        "    # check the cache\n",
        "    if os.path.exists(os.path.join(gdrive_workspace, prefix + \"_filenames.txt\")):\n",
        "      copyfile(os.path.join(gdrive_workspace, prefix + \"_filenames.txt\"), \n",
        "               os.path.join(colab_data_dir, prefix + \"_filenames.txt\"))\n",
        "    else:\n",
        "      # generate from scratch\n",
        "      utils.generate_filenames(os.path.join(colab_data_dir, prefix))\n",
        "      copyfile(os.path.join(colab_data_dir, prefix + \"_filenames.txt\"), \n",
        "               os.path.join(gdrive_workspace, prefix + \"_filenames.txt\"))\n",
        "    \n",
        "\n",
        "  \n",
        "print(\"Filenames files are ready.\")\n",
        "!ls -l {colab_data_dir}"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filenames files are ready.\n",
            "total 4688\n",
            "drwxr-xr-x 12 root root    4096 Nov 25  2015 300W_LP\n",
            "-rw-r--r--  1 root root 4618569 Aug 13 12:21 300W_LP_filenames.txt\n",
            "drwxr-xr-x  3 root root  135168 Mar 29  2016 AFLW2000\n",
            "-rw-r--r--  1 root root   39380 Aug 13 12:27 AFLW2000_filenames.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs2AVhGOerAc",
        "colab_type": "text"
      },
      "source": [
        "# Training as is\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFrtaMc5N4nV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "384a8871-b68e-4c16-a160-ab3f596fb07b"
      },
      "source": [
        "# batch size is 128, see https://github.com/natanielruiz/deep-head-pose/issues/55\n",
        "\n",
        "# filename_list = os.path.join(colab_data_dir, traindata_prefix) + \"_filenames.txt\"\n",
        "# !python train_hopenet.py --num_epochs 2 --batch_size 128 --lr 10e-5 --dataset Pose_300W_LP --data_dir /content/data --alpha 1 --filename_list {filename_list}\n",
        "\n",
        "filename_list = os.path.join(colab_data_dir, testdata_prefix) + \"_filenames.txt\"\n",
        "!python train_hopenet.py --num_epochs 3 --batch_size 128 --lr 10e-5 --dataset AFLW2000 --data_dir /content/data --alpha 1 --filename_list {filename_list}\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data.\n",
            "Ready to train network.\n",
            "Epoch completed in 20.3 seconds. Taking snapshot...\n",
            "Epoch completed in 20.5 seconds. Taking snapshot...\n",
            "Epoch completed in 20.7 seconds. Taking snapshot...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYXph7EKq5St",
        "colab_type": "code",
        "outputId": "cfedfdfb-c147-4ec7-ac7f-eb237d26cfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls /content/deep-head-pose/code/output/snapshots"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_epoch_1.pkl  _epoch_2.pkl  _epoch_3.pkl  losses.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvK7z4NG0LFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e721b424-4d3f-42c5-f953-909f887f6cf6"
      },
      "source": [
        "import torch\n",
        "torch.cuda.current_device()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UTzsm2ycZYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzdpNTs0i36F",
        "colab_type": "text"
      },
      "source": [
        "# Display Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J8TDiNWi6Om",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e70bd493-e091-438b-b120-1e2e98cf39ec"
      },
      "source": [
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('classic')\n",
        "%matplotlib inline\n",
        "\n",
        "loss_file = \"/content/deep-head-pose/code/output/snapshots/losses.pkl\"\n",
        "with open(loss_file, \"rb\") as file_handle:\n",
        "  training_stats = pkl.load(file_handle)\n",
        "  \n",
        "print(len(training_stats[\"loss_yaw\"]))\n",
        "\n",
        "plt.plot(training_stats[\"loss_yaw\"], label=\"Yaw\")\n",
        "plt.plot(training_stats[\"loss_pitch\"], label=\"Pitch\")\n",
        "plt.plot(training_stats[\"loss_roll\"], label=\"Roll\")\n",
        "            "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n",
            "48\n",
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qmsyZahjl7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}